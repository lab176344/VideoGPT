{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXfZXsNhy08r"
   },
   "source": [
    "# Using VideoGPT\n",
    "This is a notebook demonstrating how to use VideoGPT and any pretrained models, Make sure that it is a GPU instance: **Change Runtime Type -> GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ponLMda7zBmF"
   },
   "source": [
    "## Installation\n",
    "First, we install the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KD86SgJ1yvwZ",
    "outputId": "e1b99c2b-9ddb-4956-b103-0c506feed76b"
   },
   "outputs": [],
   "source": [
    "! pip install torchvision\r\n",
    "! pip install git+https://github.com/wilson1yan/VideoGPT.git\r\n",
    "! pip install scikit-video av\r\n",
    "\r\n",
    "# Shifts src_tf dim to dest dim\r\n",
    "# i.e. shift_dim(x, 1, -1) would be (b, c, t, h, w) -> (b, t, h, w, c)\r\n",
    "def shift_dim(x, src_dim=-1, dest_dim=-1, make_contiguous=True):\r\n",
    "    n_dims = len(x.shape)\r\n",
    "    if src_dim < 0:\r\n",
    "        src_dim = n_dims + src_dim\r\n",
    "    if dest_dim < 0:\r\n",
    "        dest_dim = n_dims + dest_dim\r\n",
    "\r\n",
    "    assert 0 <= src_dim < n_dims and 0 <= dest_dim < n_dims\r\n",
    "\r\n",
    "    dims = list(range(n_dims))\r\n",
    "    del dims[src_dim]\r\n",
    "\r\n",
    "    permutation = []\r\n",
    "    ctr = 0\r\n",
    "    for i in range(n_dims):\r\n",
    "        if i == dest_dim:\r\n",
    "            permutation.append(src_dim)\r\n",
    "        else:\r\n",
    "            permutation.append(dims[ctr])\r\n",
    "            ctr += 1\r\n",
    "    x = x.permute(permutation)\r\n",
    "    if make_contiguous:\r\n",
    "        x = x.contiguous()\r\n",
    "    return x\r\n",
    "\r\n",
    "# reshapes tensor start from dim i (inclusive)\r\n",
    "# to dim j (exclusive) to the desired shape\r\n",
    "# e.g. if x.shape = (b, thw, c) then\r\n",
    "# view_range(x, 1, 2, (t, h, w)) returns\r\n",
    "# x of shape (b, t, h, w, c)\r\n",
    "def view_range(x, i, j, shape):\r\n",
    "    shape = tuple(shape)\r\n",
    "\r\n",
    "    n_dims = len(x.shape)\r\n",
    "    if i < 0:\r\n",
    "        i = n_dims + i\r\n",
    "\r\n",
    "    if j is None:\r\n",
    "        j = n_dims\r\n",
    "    elif j < 0:\r\n",
    "        j = n_dims + j\r\n",
    "\r\n",
    "    assert 0 <= i < j <= n_dims\r\n",
    "\r\n",
    "    x_shape = x.shape\r\n",
    "    target_shape = x_shape[:i] + shape + x_shape[j:]\r\n",
    "    return x.view(target_shape)\r\n",
    "\r\n",
    "    \r\n",
    "def tensor_slice(x, begin, size):\r\n",
    "    assert all([b >= 0 for b in begin])\r\n",
    "    size = [l - b if s == -1 else s\r\n",
    "            for s, b, l in zip(size, begin, x.shape)]\r\n",
    "    assert all([s >= 0 for s in size])\r\n",
    "\r\n",
    "    slices = [slice(b, b + s) for b, s in zip(begin, size)]\r\n",
    "    return x[slices]\r\n",
    "\r\n",
    "\r\n",
    "import math\r\n",
    "import numpy as np\r\n",
    "import skvideo.io\r\n",
    "def save_video_grid(video, fname, nrow=None):\r\n",
    "    b, c, t, h, w = video.shape\r\n",
    "    video = video.permute(0, 2, 3, 4, 1)\r\n",
    "    video = (video.cpu().numpy() * 255).astype('uint8')\r\n",
    "\r\n",
    "    if nrow is None:\r\n",
    "        nrow = math.ceil(math.sqrt(b))\r\n",
    "    ncol = math.ceil(b / nrow)\r\n",
    "    padding = 1\r\n",
    "    video_grid = np.zeros((t, (padding + h) * nrow + padding,\r\n",
    "                           (padding + w) * ncol + padding, c), dtype='uint8')\r\n",
    "    for i in range(b):\r\n",
    "        r = i // ncol\r\n",
    "        c = i % ncol\r\n",
    "\r\n",
    "        start_r = (padding + h) * r\r\n",
    "        start_c = (padding + w) * c\r\n",
    "        video_grid[:, start_r:start_r + h, start_c:start_c + w] = video[i]\r\n",
    "\r\n",
    "    skvideo.io.vwrite(fname, video_grid, inputdict={'-r': '5'})\r\n",
    "    print('saved videos to', fname)\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6G1lfDiwycLl"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torchvision.io import read_video, read_video_timestamps\n",
    "\n",
    "from videogpt import download, load_vqvae, load_videogpt\n",
    "from videogpt.data import preprocess\n",
    "\n",
    "VIDEOS = {\n",
    "    'breakdancing': '1OZBnG235-J9LgB_qHv-waHZ4tjofiDgj',\n",
    "    'bear': '16nIaqq2vbPh-WMo_7hs9feVSe0jWVXLF',\n",
    "    'jaywalking': '1UxKCVrbyXhvMz_H7dI4w5hjPpRGCAApy',\n",
    "    'cartoon': '1ONcTMSEuGuLYIDbX-KeFqd390vbTIH9d'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXN6A0sHEmzT"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AttentionStack(nn.Module):\n",
    "    def __init__(\n",
    "        self, shape, embd_dim, n_head, n_layer, dropout,\n",
    "        attn_type, attn_dropout, class_cond_dim, frame_cond_shape,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.training = True\n",
    "        self.shape = shape\n",
    "        self.embd_dim = embd_dim\n",
    "        self.use_frame_cond = frame_cond_shape is not None\n",
    "\n",
    "        self.right_shift = RightShift(embd_dim)\n",
    "        self.pos_embd = AddBroadcastPosEmbed(\n",
    "            shape=shape, embd_dim=embd_dim\n",
    "        )\n",
    "\n",
    "        self.attn_nets = nn.ModuleList(\n",
    "            [\n",
    "                AttentionBlock(\n",
    "                    shape=shape,\n",
    "                    embd_dim=embd_dim,\n",
    "                    n_head=n_head,\n",
    "                    n_layer=n_layer,\n",
    "                    dropout=dropout,\n",
    "                    attn_type=attn_type,\n",
    "                    attn_dropout=attn_dropout,\n",
    "                    class_cond_dim=class_cond_dim,\n",
    "                    frame_cond_shape=frame_cond_shape\n",
    "                )\n",
    "                for i in range(n_layer)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, cond, decode_step, decode_idx):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        ------\n",
    "            x: (b, d1, d2, ..., dn, embd_dim)\n",
    "            cond: a dictionary of conditioning tensors\n",
    "\n",
    "            (below is used only when sampling for fast decoding)\n",
    "            decode: the enumerated rasterscan order of the current idx being sampled\n",
    "            decode_step: a tuple representing the current idx being sampled\n",
    "        \"\"\"\n",
    "        x = self.right_shift(x, decode_step)\n",
    "        x = self.pos_embd(x, decode_step, decode_idx)\n",
    "        for net in self.attn_nets:\n",
    "            x = net(x, cond, decode_step, decode_idx)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, shape, embd_dim, n_head, n_layer, dropout,\n",
    "                 attn_type, attn_dropout, class_cond_dim, frame_cond_shape):\n",
    "        super().__init__()\n",
    "        self.training = True\n",
    "        self.use_frame_cond = frame_cond_shape is not None\n",
    "\n",
    "        self.pre_attn_norm = LayerNorm(embd_dim, class_cond_dim)\n",
    "        self.post_attn_dp = nn.Dropout(dropout)\n",
    "        self.attn = MultiHeadAttention(shape, embd_dim, embd_dim, n_head,\n",
    "                                       n_layer, causal=True, attn_type=attn_type,\n",
    "                                       attn_kwargs=dict(attn_dropout=attn_dropout))\n",
    "\n",
    "        if frame_cond_shape is not None:\n",
    "            enc_len = np.prod(frame_cond_shape[:-1])\n",
    "            self.pre_enc_norm = LayerNorm(embd_dim, class_cond_dim)\n",
    "            self.post_enc_dp = nn.Dropout(dropout)\n",
    "            self.enc_attn = MultiHeadAttention(shape, embd_dim, frame_cond_shape[-1],\n",
    "                                               n_head, n_layer, attn_type='full',\n",
    "                                               attn_kwargs=dict(attn_dropout=0.), causal=False)\n",
    "\n",
    "        self.pre_fc_norm = LayerNorm(embd_dim, class_cond_dim)\n",
    "        self.post_fc_dp = nn.Dropout(dropout)\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Linear(in_features=embd_dim, out_features=embd_dim * 4),\n",
    "            GeLU2(),\n",
    "            nn.Linear(in_features=embd_dim * 4, out_features=embd_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, cond, decode_step, decode_idx):\n",
    "        h = self.pre_attn_norm(x, cond)\n",
    "        if self.training:\n",
    "            h = checkpoint(self.attn, h, h, h, decode_step, decode_idx)\n",
    "        else:\n",
    "            h = self.attn(h, h, h, decode_step, decode_idx)\n",
    "        h = self.post_attn_dp(h)\n",
    "        x = x + h\n",
    "\n",
    "        if self.use_frame_cond:\n",
    "            h = self.pre_enc_norm(x, cond)\n",
    "            if self.training:\n",
    "                h = checkpoint(self.enc_attn, h, cond['frame_cond'], cond['frame_cond'],\n",
    "                               decode_step, decode_idx)\n",
    "            else:\n",
    "                h = self.enc_attn(h, cond['frame_cond'], cond['frame_cond'],\n",
    "                                  decode_step, decode_idx)\n",
    "            h = self.post_enc_dp(h)\n",
    "            x = x + h\n",
    "\n",
    "        h = self.pre_fc_norm(x, cond)\n",
    "        if self.training:\n",
    "            h = checkpoint(self.fc_block, h)\n",
    "        else:\n",
    "            h = self.fc_block(h)\n",
    "        h = self.post_fc_dp(h)\n",
    "        x = x + h\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, shape, dim_q, dim_kv, n_head, n_layer,\n",
    "                 causal, attn_type, attn_kwargs):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.shape = shape\n",
    "\n",
    "        self.d_k = dim_q // n_head\n",
    "        self.d_v = dim_kv // n_head\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.w_qs = nn.Linear(dim_q, n_head * self.d_k, bias=False) # q\n",
    "        self.w_qs.weight.data.normal_(std=1.0 / np.sqrt(dim_q))\n",
    "\n",
    "        self.w_ks = nn.Linear(dim_kv, n_head * self.d_k, bias=False) # k\n",
    "        self.w_ks.weight.data.normal_(std=1.0 / np.sqrt(dim_kv))\n",
    "\n",
    "        self.w_vs = nn.Linear(dim_kv, n_head * self.d_v, bias=False) # v\n",
    "        self.w_vs.weight.data.normal_(std=1.0 / np.sqrt(dim_kv))\n",
    "\n",
    "        self.fc = nn.Linear(n_head * self.d_v, dim_q, bias=True) # c\n",
    "        self.fc.weight.data.normal_(std=1.0 / np.sqrt(dim_q * n_layer))\n",
    "\n",
    "        if attn_type == 'full':\n",
    "            self.attn = FullAttention(shape, causal, **attn_kwargs)\n",
    "        elif attn_type == 'axial':\n",
    "            assert not causal, 'causal axial attention is not supported'\n",
    "            self.attn = AxialAttention(len(shape), **attn_kwargs)\n",
    "        elif attn_type == 'sparse':\n",
    "            self.attn = SparseAttention(shape, n_head, causal, **attn_kwargs)\n",
    "\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, q, k, v, decode_step=None, decode_idx=None):\n",
    "        \"\"\" Compute multi-head attention\n",
    "        Args\n",
    "            q, k, v: a [b, d1, ..., dn, c] tensor or\n",
    "                     a [b, 1, ..., 1, c] tensor if decode_step is not None\n",
    "\n",
    "        Returns\n",
    "            The output after performing attention\n",
    "        \"\"\"\n",
    "\n",
    "        # compute k, q, v\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "        q = view_range(self.w_qs(q), -1, None, (n_head, d_k))\n",
    "        k = view_range(self.w_ks(k), -1, None, (n_head, d_k))\n",
    "        v = view_range(self.w_vs(v), -1, None, (n_head, d_v))\n",
    "\n",
    "        # b x n_head x seq_len x d\n",
    "        # (b, *d_shape, n_head, d) -> (b, n_head, *d_shape, d)\n",
    "        q = shift_dim(q, -2, 1)\n",
    "        k = shift_dim(k, -2, 1)\n",
    "        v = shift_dim(v, -2, 1)\n",
    "\n",
    "        # fast decoding\n",
    "        if decode_step is not None:\n",
    "            if decode_step == 0:\n",
    "                if self.causal:\n",
    "                    k_shape = (q.shape[0], n_head, *self.shape, self.d_k)\n",
    "                    v_shape = (q.shape[0], n_head, *self.shape, self.d_v)\n",
    "                    self.cache = dict(k=torch.zeros(k_shape, dtype=k.dtype, device=q.device),\n",
    "                                    v=torch.zeros(v_shape, dtype=v.dtype, device=q.device))\n",
    "                else:\n",
    "                    # cache only once in the non-causal case\n",
    "                    self.cache = dict(k=k.clone(), v=v.clone())\n",
    "            if self.causal:\n",
    "                idx = (slice(None, None), slice(None, None), *[slice(i, i+ 1) for i in decode_idx])\n",
    "                self.cache['k'][idx] = k\n",
    "                self.cache['v'][idx] = v\n",
    "            k, v = self.cache['k'], self.cache['v']\n",
    "\n",
    "        a = self.attn(q, k, v, decode_step, decode_idx)\n",
    "\n",
    "        # (b, *d_shape, n_head, d) -> (b, *d_shape, n_head * d)\n",
    "        a = shift_dim(a, 1, -2).flatten(start_dim=-2)\n",
    "        a = self.fc(a) # (b x seq_len x embd_dim)\n",
    "\n",
    "        return a\n",
    "\n",
    "############## Attention #######################\n",
    "class FullAttention(nn.Module):\n",
    "    def __init__(self, shape, causal, attn_dropout):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.attn_dropout = attn_dropout\n",
    "\n",
    "        seq_len = np.prod(shape)\n",
    "        if self.causal:\n",
    "            self.register_buffer('mask', torch.tril(torch.ones(seq_len, seq_len)))\n",
    "\n",
    "    def forward(self, q, k, v, decode_step, decode_idx):\n",
    "        mask = self.mask if self.causal else None\n",
    "        if decode_step is not None and mask is not None:\n",
    "            mask = mask[[decode_step]]\n",
    "\n",
    "        old_shape = q.shape[2:-1]\n",
    "        q = q.flatten(start_dim=2, end_dim=-2)\n",
    "        k = k.flatten(start_dim=2, end_dim=-2)\n",
    "        v = v.flatten(start_dim=2, end_dim=-2)\n",
    "\n",
    "        out = scaled_dot_product_attention(q, k, v, mask=mask,\n",
    "                                           attn_dropout=self.attn_dropout,\n",
    "                                           training=self.training)\n",
    "\n",
    "        return view_range(out, 2, 3, old_shape)\n",
    "\n",
    "class AxialAttention(nn.Module):\n",
    "    def __init__(self, n_dim, axial_dim):\n",
    "        super().__init__()\n",
    "        if axial_dim < 0:\n",
    "            axial_dim = 2 + n_dim + 1 + axial_dim\n",
    "        else:\n",
    "            axial_dim += 2 # account for batch, head, dim\n",
    "        self.axial_dim = axial_dim\n",
    "\n",
    "    def forward(self, q, k, v, decode_step, decode_idx):\n",
    "        q = shift_dim(q, self.axial_dim, -2).flatten(end_dim=-3)\n",
    "        k = shift_dim(k, self.axial_dim, -2).flatten(end_dim=-3)\n",
    "        v = shift_dim(v, self.axial_dim, -2)\n",
    "        old_shape = list(v.shape)\n",
    "        v = v.flatten(end_dim=-3)\n",
    "\n",
    "        out = scaled_dot_product_attention(q, k, v, training=self.training)\n",
    "        out = out.view(*old_shape)\n",
    "        out = shift_dim(out, -2, self.axial_dim)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SparseAttention(nn.Module):\n",
    "    ops = dict()\n",
    "    attn_mask = dict()\n",
    "    block_layout = dict()\n",
    "\n",
    "    def __init__(self, shape, n_head, causal, num_local_blocks=4, block=32,\n",
    "                 attn_dropout=0.): # does not use attn_dropout\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.shape = shape\n",
    "\n",
    "        self.sparsity_config = StridedSparsityConfig(shape=shape, n_head=n_head,\n",
    "                                                     causal=causal, block=block,\n",
    "                                                     num_local_blocks=num_local_blocks)\n",
    "\n",
    "        if self.shape not in SparseAttention.block_layout:\n",
    "            SparseAttention.block_layout[self.shape] = self.sparsity_config.make_layout()\n",
    "        if causal and self.shape not in SparseAttention.attn_mask:\n",
    "            SparseAttention.attn_mask[self.shape] = self.sparsity_config.make_sparse_attn_mask()\n",
    "\n",
    "    def get_ops(self):\n",
    "        try:\n",
    "            from deepspeed.ops.sparse_attention import MatMul, Softmax\n",
    "        except:\n",
    "            raise Exception('Error importing deepspeed. Please install using `DS_BUILD_SPARSE_ATTN=1 pip install deepspeed`')\n",
    "        if self.shape not in SparseAttention.ops:\n",
    "            sparsity_layout = self.sparsity_config.make_layout()\n",
    "            sparse_dot_sdd_nt = MatMul(sparsity_layout,\n",
    "                                       self.sparsity_config.block,\n",
    "                                       'sdd',\n",
    "                                       trans_a=False,\n",
    "                                       trans_b=True)\n",
    "\n",
    "            sparse_dot_dsd_nn = MatMul(sparsity_layout,\n",
    "                                       self.sparsity_config.block,\n",
    "                                       'dsd',\n",
    "                                       trans_a=False,\n",
    "                                       trans_b=False)\n",
    "\n",
    "            sparse_softmax = Softmax(sparsity_layout, self.sparsity_config.block)\n",
    "\n",
    "            SparseAttention.ops[self.shape] = (sparse_dot_sdd_nt,\n",
    "                                               sparse_dot_dsd_nn,\n",
    "                                               sparse_softmax)\n",
    "        return SparseAttention.ops[self.shape]\n",
    "\n",
    "    def forward(self, q, k, v, decode_step, decode_idx):\n",
    "        if self.training and self.shape not in SparseAttention.ops:\n",
    "            self.get_ops()\n",
    "\n",
    "        SparseAttention.block_layout[self.shape] = SparseAttention.block_layout[self.shape].to(q)\n",
    "        if self.causal:\n",
    "            SparseAttention.attn_mask[self.shape] = SparseAttention.attn_mask[self.shape].to(q).type_as(q)\n",
    "        attn_mask = SparseAttention.attn_mask[self.shape] if self.causal else None\n",
    "\n",
    "        old_shape = q.shape[2:-1]\n",
    "        q = q.flatten(start_dim=2, end_dim=-2)\n",
    "        k = k.flatten(start_dim=2, end_dim=-2)\n",
    "        v = v.flatten(start_dim=2, end_dim=-2)\n",
    "\n",
    "        if decode_step is not None:\n",
    "            mask = self.sparsity_config.get_non_block_layout_row(SparseAttention.block_layout[self.shape], decode_step)\n",
    "            out = scaled_dot_product_attention(q, k, v, mask=mask, training=self.training)\n",
    "        else:\n",
    "            if q.shape != k.shape or k.shape != v.shape:\n",
    "                raise Exception('SparseAttention only support self-attention')\n",
    "            sparse_dot_sdd_nt, sparse_dot_dsd_nn, sparse_softmax = self.get_ops()\n",
    "            scaling = float(q.shape[-1]) ** -0.5\n",
    "\n",
    "            attn_output_weights = sparse_dot_sdd_nt(q, k)\n",
    "            if attn_mask is not None:\n",
    "                attn_output_weights = attn_output_weights.masked_fill(attn_mask == 0,\n",
    "                                                                      float('-inf'))\n",
    "            attn_output_weights = sparse_softmax(\n",
    "                attn_output_weights,\n",
    "                scale=scaling\n",
    "            )\n",
    "\n",
    "            out = sparse_dot_dsd_nn(attn_output_weights, v)\n",
    "\n",
    "        return view_range(out, 2, 3, old_shape)\n",
    "\n",
    "\n",
    "class StridedSparsityConfig(object):\n",
    "    \"\"\"\n",
    "    Strided Sparse configuration specified in https://arxiv.org/abs/1904.10509 that\n",
    "    generalizes to arbitrary dimensions\n",
    "    \"\"\"\n",
    "    def __init__(self, shape, n_head, causal, block, num_local_blocks):\n",
    "        self.n_head = n_head\n",
    "        self.shape = shape\n",
    "        self.causal = causal\n",
    "        self.block = block\n",
    "        self.num_local_blocks = num_local_blocks\n",
    "\n",
    "        assert self.num_local_blocks >= 1, 'Must have at least 1 local block'\n",
    "        assert self.seq_len % self.block == 0, 'seq len must be divisible by block size'\n",
    "\n",
    "        self._block_shape = self._compute_block_shape()\n",
    "        self._block_shape_cum = self._block_shape_cum_sizes()\n",
    "\n",
    "    @property\n",
    "    def seq_len(self):\n",
    "        return np.prod(self.shape)\n",
    "\n",
    "    @property\n",
    "    def num_blocks(self):\n",
    "        return self.seq_len // self.block\n",
    "\n",
    "    def set_local_layout(self, layout):\n",
    "        num_blocks = self.num_blocks\n",
    "        for row in range(0, num_blocks):\n",
    "            end = min(row + self.num_local_blocks, num_blocks)\n",
    "            for col in range(\n",
    "                    max(0, row - self.num_local_blocks),\n",
    "                    (row + 1 if self.causal else end)):\n",
    "                layout[:, row, col] = 1\n",
    "        return layout\n",
    "\n",
    "    def set_global_layout(self, layout):\n",
    "        num_blocks = self.num_blocks\n",
    "        n_dim = len(self._block_shape)\n",
    "        for row in range(num_blocks):\n",
    "            assert self._to_flattened_idx(self._to_unflattened_idx(row)) == row\n",
    "            cur_idx = self._to_unflattened_idx(row)\n",
    "            # no strided attention over last dim\n",
    "            for d in range(n_dim - 1):\n",
    "                end = self._block_shape[d]\n",
    "                for i in range(0, (cur_idx[d] + 1 if self.causal else end)):\n",
    "                    new_idx = list(cur_idx)\n",
    "                    new_idx[d] = i\n",
    "                    new_idx = tuple(new_idx)\n",
    "\n",
    "                    col = self._to_flattened_idx(new_idx)\n",
    "                    layout[:, row, col] = 1\n",
    "\n",
    "        return layout\n",
    "\n",
    "    def make_layout(self):\n",
    "        layout = torch.zeros((self.n_head, self.num_blocks, self.num_blocks), dtype=torch.int64)\n",
    "        layout = self.set_local_layout(layout)\n",
    "        layout = self.set_global_layout(layout)\n",
    "        return layout\n",
    "\n",
    "    def make_sparse_attn_mask(self):\n",
    "        block_layout = self.make_layout()\n",
    "        assert block_layout.shape[1] == block_layout.shape[2] == self.num_blocks\n",
    "\n",
    "        num_dense_blocks = block_layout.sum().item()\n",
    "        attn_mask = torch.ones(num_dense_blocks, self.block, self.block)\n",
    "        counter = 0\n",
    "        for h in range(self.n_head):\n",
    "            for i in range(self.num_blocks):\n",
    "                for j in range(self.num_blocks):\n",
    "                    elem = block_layout[h, i, j].item()\n",
    "                    if elem == 1:\n",
    "                        assert i >= j\n",
    "                        if i == j: # need to mask within block on diagonals\n",
    "                            attn_mask[counter] = torch.tril(attn_mask[counter])\n",
    "                        counter += 1\n",
    "        assert counter == num_dense_blocks\n",
    "\n",
    "        return attn_mask.unsqueeze(0)\n",
    "\n",
    "    def get_non_block_layout_row(self, block_layout, row):\n",
    "        block_row = row // self.block\n",
    "        block_row = block_layout[:, [block_row]] # n_head x 1 x n_blocks\n",
    "        block_row = block_row.repeat_interleave(self.block, dim=-1)\n",
    "        block_row[:, :, row + 1:] = 0.\n",
    "        return block_row\n",
    "\n",
    "    ############# Helper functions ##########################\n",
    "\n",
    "    def _compute_block_shape(self):\n",
    "        n_dim = len(self.shape)\n",
    "        cum_prod = 1\n",
    "        for i in range(n_dim - 1, -1, -1):\n",
    "            cum_prod *= self.shape[i]\n",
    "            if cum_prod > self.block:\n",
    "                break\n",
    "        assert cum_prod % self.block == 0\n",
    "        new_shape = (*self.shape[:i], cum_prod // self.block)\n",
    "\n",
    "        assert np.prod(new_shape) == np.prod(self.shape) // self.block\n",
    "\n",
    "        return new_shape\n",
    "\n",
    "    def _block_shape_cum_sizes(self):\n",
    "        bs = np.flip(np.array(self._block_shape))\n",
    "        return tuple(np.flip(np.cumprod(bs)[:-1])) + (1,)\n",
    "\n",
    "    def _to_flattened_idx(self, idx):\n",
    "        assert len(idx) == len(self._block_shape), f\"{len(idx)} != {len(self._block_shape)}\"\n",
    "        flat_idx = 0\n",
    "        for i in range(len(self._block_shape)):\n",
    "            flat_idx += idx[i] * self._block_shape_cum[i]\n",
    "        return flat_idx\n",
    "\n",
    "    def _to_unflattened_idx(self, flat_idx):\n",
    "        assert flat_idx < np.prod(self._block_shape)\n",
    "        idx = []\n",
    "        for i in range(len(self._block_shape)):\n",
    "            idx.append(flat_idx // self._block_shape_cum[i])\n",
    "            flat_idx %= self._block_shape_cum[i]\n",
    "        return tuple(idx)\n",
    "\n",
    "\n",
    "################ Spatiotemporal broadcasted positional embeddings ###############\n",
    "class AddBroadcastPosEmbed(nn.Module):\n",
    "    def __init__(self, shape, embd_dim, dim=-1):\n",
    "        super().__init__()\n",
    "        assert dim in [-1, 1] # only first or last dim supported\n",
    "        self.shape = shape\n",
    "        self.n_dim = n_dim = len(shape)\n",
    "        self.embd_dim = embd_dim\n",
    "        self.dim = dim\n",
    "\n",
    "        assert embd_dim % n_dim == 0, f\"{embd_dim} % {n_dim} != 0\"\n",
    "        self.emb = nn.ParameterDict({\n",
    "             f'd_{i}': nn.Parameter(torch.randn(shape[i], embd_dim // n_dim) * 0.01\n",
    "                                    if dim == -1 else\n",
    "                                    torch.randn(embd_dim // n_dim, shape[i]) * 0.01)\n",
    "             for i in range(n_dim)\n",
    "        })\n",
    "\n",
    "    def forward(self, x, decode_step=None, decode_idx=None):\n",
    "        embs = []\n",
    "        for i in range(self.n_dim):\n",
    "            e = self.emb[f'd_{i}']\n",
    "            if self.dim == -1:\n",
    "                # (1, 1, ..., 1, self.shape[i], 1, ..., -1)\n",
    "                e = e.view(1, *((1,) * i), self.shape[i], *((1,) * (self.n_dim - i - 1)), -1)\n",
    "                e = e.expand(1, *self.shape, -1)\n",
    "            else:\n",
    "                e = e.view(1, -1, *((1,) * i), self.shape[i], *((1,) * (self.n_dim - i - 1)))\n",
    "                e = e.expand(1, -1, *self.shape)\n",
    "            embs.append(e)\n",
    "\n",
    "        embs = torch.cat(embs, dim=self.dim)\n",
    "        if decode_step is not None:\n",
    "            embs = tensor_slice(embs, [0, *decode_idx, 0],\n",
    "                                [x.shape[0], *(1,) * self.n_dim, x.shape[-1]])\n",
    "\n",
    "        return x + embs\n",
    "\n",
    "################# Helper Functions ###################################\n",
    "def scaled_dot_product_attention(q, k, v, mask=None, attn_dropout=0., training=True):\n",
    "    # Performs scaled dot-product attention over the second to last dimension dn\n",
    "\n",
    "    # (b, n_head, d1, ..., dn, d)\n",
    "    attn = torch.matmul(q, k.transpose(-1, -2))\n",
    "    attn = attn / np.sqrt(q.shape[-1])\n",
    "    if mask is not None:\n",
    "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
    "    attn_float = F.softmax(attn, dim=-1)\n",
    "    attn = attn_float.type_as(attn) # b x n_head x d1 x ... x dn x d\n",
    "    attn = F.dropout(attn, p=attn_dropout, training=training)\n",
    "\n",
    "    a = torch.matmul(attn, v) # b x n_head x d1 x ... x dn x d\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "class RightShift(nn.Module):\n",
    "    def __init__(self, embd_dim):\n",
    "        super().__init__()\n",
    "        self.embd_dim = embd_dim\n",
    "        self.sos = nn.Parameter(torch.FloatTensor(embd_dim).normal_(std=0.02), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, decode_step):\n",
    "        if decode_step is not None and decode_step > 0:\n",
    "            return x\n",
    "\n",
    "        x_shape = list(x.shape)\n",
    "        x = x.flatten(start_dim=1, end_dim=-2) # (b, seq_len, embd_dim)\n",
    "        sos = torch.ones(x_shape[0], 1, self.embd_dim, dtype=torch.float32).to(self.sos) * self.sos\n",
    "        sos = sos.type_as(x)\n",
    "        x = torch.cat([sos, x[:, :-1, :]], axis=1)\n",
    "        x = x.view(*x_shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeLU2(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return (1.702 * x).sigmoid() * x\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embd_dim, class_cond_dim):\n",
    "        super().__init__()\n",
    "        self.conditional = class_cond_dim is not None\n",
    "\n",
    "        if self.conditional:\n",
    "            self.w = nn.Linear(class_cond_dim, embd_dim, bias=False)\n",
    "            nn.init.constant_(self.w.weight.data, 1. / np.sqrt(class_cond_dim))\n",
    "            self.wb = nn.Linear(class_cond_dim, embd_dim, bias=False)\n",
    "        else:\n",
    "            self.g = nn.Parameter(torch.ones(embd_dim, dtype=torch.float32), requires_grad=True)\n",
    "            self.b = nn.Parameter(torch.zeros(embd_dim, dtype=torch.float32), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        if self.conditional:  # (b, cond_dim)\n",
    "            g = 1 + self.w(cond['class_cond']).view(x.shape[0], *(1,)*(len(x.shape)-2), x.shape[-1]) # (b, ..., embd_dim)\n",
    "            b = self.wb(cond['class_cond']).view(x.shape[0], *(1,)*(len(x.shape)-2), x.shape[-1])\n",
    "        else:\n",
    "            g = self.g  # (embd_dim,)\n",
    "            b = self.b\n",
    "\n",
    "        x_float = x.float()\n",
    "\n",
    "        mu = x_float.mean(dim=-1, keepdims=True)\n",
    "        s = (x_float - mu).square().mean(dim=-1, keepdims=True)\n",
    "        x_float = (x_float - mu) * (1e-5 + s.rsqrt())  # (b, ..., embd_dim)\n",
    "        x_float = x_float * g + b\n",
    "\n",
    "        x = x_float.type_as(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_RyF9AU2tzK"
   },
   "source": [
    "## Downloading a Pretrained VQ-VAE\n",
    "There are four pretrained models available: `bair_stride4x2x2`, `ucf101_stride4x4x4`, `kinetics_stride4x4x4`, and `kinetics_stride2x4x4`. BAIR was trained on 64 x 64 video, and the rest on 128 x 128. The `stride` component represents the THW downsampling the VQ-VAE performs on the video tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4t5fEML30L3f",
    "outputId": "d84c1b92-954c-4e49-9cf8-bd7cfe40b854"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "vqvae = load_vqvae('kinetics_stride2x4x4', device=device).to(device)\n",
    "print(vqvae.latent_shape)\n",
    "print(vqvae.embedding_dim)\n",
    "\n",
    "def shift_dim(x, src_dim=-1, dest_dim=-1, make_contiguous=True):\n",
    "    n_dims = len(x.shape)\n",
    "    if src_dim < 0:\n",
    "        src_dim = n_dims + src_dim\n",
    "    if dest_dim < 0:\n",
    "        dest_dim = n_dims + dest_dim\n",
    "\n",
    "    assert 0 <= src_dim < n_dims and 0 <= dest_dim < n_dims\n",
    "\n",
    "    dims = list(range(n_dims))\n",
    "    del dims[src_dim]\n",
    "\n",
    "    permutation = []\n",
    "    ctr = 0\n",
    "    for i in range(n_dims):\n",
    "        if i == dest_dim:\n",
    "            permutation.append(src_dim)\n",
    "        else:\n",
    "            permutation.append(dims[ctr])\n",
    "            ctr += 1\n",
    "    x = x.permute(permutation)\n",
    "    if make_contiguous:\n",
    "        x = x.contiguous()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QMgNCPo3jQg"
   },
   "source": [
    "## Video Loading and Preprocessing\n",
    "The code below downloads, loads, and preprocesses a given `mp4` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_FYfsIf2kwU",
    "outputId": "ae164fa4-63bc-415e-fbdb-01176876f6f6"
   },
   "outputs": [],
   "source": [
    "video_name = 'jaywalking'\n",
    "# `resolution` must be divisible by the encoder image stride\n",
    "# `sequence_length` must be divisible by the encoder temporal stride\n",
    "resolution, sequence_length = vqvae.args.resolution, 16\n",
    "\n",
    "video_filename = download(VIDEOS[video_name], f'{video_name}.mp4')\n",
    "pts = read_video_timestamps(video_filename, pts_unit='sec')[0]\n",
    "video1 = read_video(video_filename, pts_unit='sec', start_pts=pts[0], end_pts=pts[sequence_length - 1])[0]\n",
    "video1 = preprocess(video1, resolution, sequence_length).unsqueeze(0).to(device)\n",
    "video_name = 'bear'\n",
    "# `resolution` must be divisible by the encoder image stride\n",
    "# `sequence_length` must be divisible by the encoder temporal stride\n",
    "resolution, sequence_length = vqvae.args.resolution, 16\n",
    "\n",
    "video_filename = download(VIDEOS[video_name], f'{video_name}.mp4')\n",
    "pts = read_video_timestamps(video_filename, pts_unit='sec')[0]\n",
    "video2 = read_video(video_filename, pts_unit='sec', start_pts=pts[0], end_pts=pts[sequence_length - 1])[0]\n",
    "video2 = preprocess(video2, resolution, sequence_length).unsqueeze(0).to(device)\n",
    "video = torch.cat((video1,video2),dim=0)\n",
    "print(video.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rA3R-ZOi3uri"
   },
   "source": [
    "## VQ-VAE Encoding and Decoding\n",
    "Now, we can encode the video through the `encode` function. The `encode` function also has an optional input `including_embeddings` (default `False`) which will also return the embedding versions of the encodings. Inlcude encodings will give the full code book vector KxCxHxW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ywTjc5wi2odm",
    "outputId": "58efc3be-aefa-4411-85a4-85a0725db564"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoding,targets = vqvae.encode(video, include_embeddings=True)\n",
    "    x = shift_dim(encoding, 1, -1)\n",
    "    print(x.shape)\n",
    "    print(encodings.shape)\n",
    "    print(targets.shape)\n",
    "    print(vqvae.n_codes)\n",
    "    print(vqvae.latent_shape)\n",
    "    video_recon = vqvae.decode(encoding)\n",
    "    video_recon = torch.clamp(video_recon, -0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrbV7CVT20V-",
    "outputId": "ad323482-5043-4c2d-bd88-21a10badd63f"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "with torch.no_grad():\n",
    "    targets,x = vqvae.encode(video, include_embeddings=True)\n",
    "    x = shift_dim(x, 1, -1)\n",
    "    print(x.shape)\n",
    "    print(targets.shape)\n",
    "hidden_dim = 576\n",
    "shape = (8, 32, 32)\n",
    "heads = 4\n",
    "layers = 8\n",
    "dropout = 0.2\n",
    "attn_type = 'full'\n",
    "attn_dropout = 0.3\n",
    "n_codes = 2048\n",
    "class_cond_dim = 10\n",
    "frame_cond_shape = None\n",
    "in_fc = nn.Linear(256, 576, bias=False)\n",
    "in_fc.weight.data.normal_(std=0.02)\n",
    "attn_stack = AttentionStack(\n",
    "    shape, hidden_dim, heads, layers, dropout,\n",
    "    attn_type, attn_dropout, class_cond_dim, frame_cond_shape\n",
    ")\n",
    "\n",
    "norm = LayerNorm(hidden_dim, class_cond_dim)\n",
    "\n",
    "fc_out = nn.Linear(hidden_dim, n_codes, bias=False)\n",
    "fc_out.weight.data.copy_(torch.zeros(n_codes, hidden_dim))\n",
    "cond = dict()\n",
    "label = torch.tensor((3,4))\n",
    "cond['class_cond'] = F.one_hot(label, class_cond_dim).type_as(x)\n",
    "print(cond['class_cond'])\n",
    "code = in_fc(x)\n",
    "h = attn_stack(code, cond, decode_step=None, decode_idx=None)\n",
    "h = norm(h, cond)\n",
    "logits = fc_out(h)\n",
    "logits = shift_dim(logits, -1, 1)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yddvcNXP_anf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcyzbBVX4J-d"
   },
   "source": [
    "## Visualizing Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "id": "Y2t-dwme2qN1",
    "outputId": "4a2cee14-9b92-45a9-a66d-358c9361377c"
   },
   "outputs": [],
   "source": [
    "videos = torch.cat((video, video_recon), dim=-1)\n",
    "videos = videos[0].permute(1, 2, 3, 0) # CTHW -> THWC\n",
    "videos = ((videos + 0.5) * 255).cpu().numpy().astype('uint8')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title('real (left), reconstruction (right)')\n",
    "plt.axis('off')\n",
    "im = plt.imshow(videos[0, :, :, :])\n",
    "plt.close()\n",
    "\n",
    "def init():\n",
    "    im.set_data(videos[0, :, :, :])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(videos[i, :, :, :])\n",
    "    return im\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=videos.shape[0], interval=200) # 200ms = 5 fps\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz3EFC-ZXL7z"
   },
   "source": [
    "# Using Pretrained VideoGPT Models\n",
    "\n",
    "The current available model to download is `ucf101`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95FevjM0XG_y",
    "outputId": "0f4c4bba-d538-44a7-b4c7-5f996709033f"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "gpt = load_videogpt('ucf101_uncond_gpt', device=device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMlpXebRY3P5"
   },
   "source": [
    "`VideoGPT.sample` method return generated sample of shape BCTHW in the range [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3o2aPtJQXnjX",
    "outputId": "a7b2321a-4bf9-450a-8dcc-beebbcc8456d"
   },
   "outputs": [],
   "source": [
    "samples = gpt.sample(16) # unconditional model does not require batch input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "id": "WZxFIFYzY_Kj",
    "outputId": "d96e310d-82c4-4d26-9b73-c7c58bcb1b74"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "b, c, t, h, w = samples.shape\n",
    "samples = samples.permute(0, 2, 3, 4, 1)\n",
    "samples = (samples.cpu().numpy() * 255).astype('uint8')\n",
    "\n",
    "video = np.zeros((t, (1 + h) * 4 + 1, (1 + w) * 4 + 1, c), dtype='uint8')\n",
    "for i in range(b):\n",
    "  r, c = i // 4, i % 4\n",
    "  start_r, start_c = (1 + h) * r, (1 + w) * c\n",
    "  video[:, start_r:start_r + h, start_c:start_c + w] = samples[i]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title('ucf101 unconditional samples')\n",
    "plt.axis('off')\n",
    "im = plt.imshow(video[0, :, :, :])\n",
    "plt.close()\n",
    "\n",
    "def init():\n",
    "    im.set_data(video[0, :, :, :])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(video[i, :, :, :])\n",
    "    return im\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0], interval=200) # 200ms = 5 fps\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCznGxs-lYNq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VideoGPT.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "3489e9235394a0d3452a89481cd738337f33d93dd937554b9907318163a22cf3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('envvgpt': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}